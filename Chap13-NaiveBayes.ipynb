{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many words: w_1, w_2, ... w_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_i: the message contains w_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(X_i|S): prob. a spam message contains w_i\n",
    "\n",
    "P(X_i|~S): prob. a nonspam message contains w_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The key to Naive Bayes is making the (big) assumption that the presences (or absences) of each word are independent of one another, conditional on a message being spam or not.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The Naive Bayes assumption allows us to compute each of the probabilities on the right simply by multiplying together the individual probability estimates for each vocabulary word.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(S|X=x) = P(X=x|S)/\\[P(X=x|S) + P(X=x|~S)\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In practice, you usually want to avoid multiplying lots of probabilities together, to avoid a problem called underflow, in which computers don’t deal well with floating-point numbers that are too close to zero. Recalling from algebra that *log ab = log a + log b* and that *exp log x = x*, we usually compute p 1 * ⋯ * p n as the equivalent (but floating-point-friendlier):\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exp (log p1 + ⋯ + log pn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The only challenge left is coming up with estimates for P(X_i|S) and P(X_i|~S), the probabilities that a spam message (or nonspam message) contains the word w_i . If we have a fair number of 'training' messages labeled as spam and not-spam, an obvious\n",
    "first try is to estimate P(X_i|S) simply as the fraction of spam messages containing word w_i.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This causes a big problem, though. Imagine that in our training set the vocabulary word “data” only occurs in nonspam messages. Then we’d estimate P(“data”|S) = 0. The result is that our Naive Bayes classifier would always assign spam probability 0 to any message containing the word “data,” even a message like “data on cheap viagra and authentic rolex watches.” To avoid this problem, we usually use some kind of smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In particular, we’ll choose a pseudocount — *k* — and estimate the probability of seeing the ith word in a spam as:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(X_i|S) = (k + number of spams containing w_i)/(2k + number of spams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"Similarly for P(X_i|~S). That is, when computing the spam probabilities for the ith word, we assume we also saw k additional spams containing the word and k additional spams not containing the word.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(message):\n",
    "    # convert to lowercase\n",
    "    message = message.lower()\n",
    "    # extract the words\n",
    "    all_words = re.findall(\"[a-z0-9']+\", message)\n",
    "    # remove duplicates\n",
    "    return set(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Our second function will count the words in a labeled training set of messages. We’ll have it return a dictionary whose keys are words, and whose values are two-element lists \\[spam_count, non_spam_count\\] corresponding to how many times we saw that\n",
    "word in both spam and nonspam messages:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(training_set):\n",
    "    \"\"\"\n",
    "    training set consists of pairs (message, is_spam)\n",
    "    \"\"\"\n",
    "    counts = defaultdict(lambda: [0, 0])\n",
    "    for message, is_spam in training_set:\n",
    "        for word in tokenize(message):\n",
    "            counts[word][0 if is_spam else 1] += 1\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_probabilities(counts, total_spams, total_non_spams, k=0.5):\n",
    "    \"\"\"\n",
    "    turn the word_counts into a list of triplets\n",
    "    w, p(w | spam) and p(w | ~spam)\n",
    "    \"\"\"\n",
    "    return [(w,\n",
    "             (spam + k) / (total_spams + 2 * k),\n",
    "             (non_spam + k) / (total_non_spams + 2 * k))\n",
    "            for w, (spam, non_spam) in counts.iteritems()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spam_probability(word_probs, message):\n",
    "    message_words = tokenize(message)\n",
    "    log_prob_if_spam = log_prob_if_not_spam = 0.0\n",
    "    # iterate through each word in our vocabulary\n",
    "    for word, prob_if_spam, prob_if_not_spam in word_probs:\n",
    "        # if *word* appears in the message,\n",
    "        # add the log probability of seeing it\n",
    "        if word in message_words:\n",
    "            log_prob_if_spam += math.log(prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(prob_if_not_spam)\n",
    "        # if *word* doesn't appear in the message\n",
    "        # add the log probability of _not_ seeing it\n",
    "        # which is log(1 - probability of seeing it)\n",
    "        else:\n",
    "            log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(1.0 - prob_if_not_spam)\n",
    "\n",
    "    prob_if_spam = math.exp(log_prob_if_spam)\n",
    "    prob_if_not_spam = math.exp(log_prob_if_not_spam)\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, k=0.5):\n",
    "        self.k = k\n",
    "        self.word_probs = []\n",
    "\n",
    "    def train(self, training_set):\n",
    "        # count spam and non-spam messages\n",
    "        num_spams = len([is_spam \n",
    "                         for message, is_spam in training_set \n",
    "                         if is_spam])\n",
    "        num_non_spams = len(training_set) - num_spams\n",
    "        # run training data through our \"pipeline\"\n",
    "        word_counts = count_words(training_set)\n",
    "        self.word_probs = word_probabilities(word_counts,num_spams,\n",
    "                                             num_non_spams,self.k)\n",
    "\n",
    "    def classify(self, message):\n",
    "        return spam_probability(self.word_probs, message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"How could we get better performance?\n",
    "* One obvious way would be to get more data to train on.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"There are a number of ways to improve the model as well.\"\n",
    "* Our classifier takes into account every word that appears in the training set, even words that appear only once. Modify the classifier to accept an optional *min_count* threshhold and ignore tokens that don’t appear at least that many times.\n",
    "* The tokenizer has no notion of similar words (e.g., “cheap” and “cheapest”). Modify the classifier to take an optional stemmer function that converts words to equivalence classes of words. For example, a really simple stemmer function might be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_final_s(word):\n",
    "    return re.sub(\"s$\", \"\", word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Creating a good stemmer function is hard. People frequently use the **Porter Stemmer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Although our features are all of the form “message contains word w_i ,” there’s no reason why this has to be the case. In our implementation, we could add extra features like “message contains a number” by creating phony tokens like contains:number and modifying the tokenizer to emit them when appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"*scikit-learn* contains a BernoulliNB model that implements the same Naive Bayes algorithm we implemented here, as well as other variations on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
