{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"A specification of a mathematical (or probabilistic) relationship that exists between different variables.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"refer\\[s\\] to creating and using models that are learned from data.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"producing a model that performs well on the data you train in on but that generalizes poorly to any new data.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"This could involve learning *noise* in the data.\"\n",
    "* \"Or this could involve learning to identify specific inputs rather that whatever factors are actually predictive for the desired output.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The other side of it is underfitting, producing a model that doesn't perform well even on the training data, although typically when this happens you decide your model isn't good enough and keep looking for a better one.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"So how do we make sure our models aren't too complex?\"\n",
    "- \"The most fundamental approach involves using different data to train the model and to test the model.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The simplest way to do this is to split your data set, so that (for example) two-thirds of it is used to train the model, after which we measure the model's performance on the remaining third:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data,prob):\n",
    "    \"\"\"\n",
    "    Splits data into [prob, 1-prob]\n",
    "    \"\"\"\n",
    "    results = [], []\n",
    "    for row in data:\n",
    "        results[0 if random.random() < prob else 1].append(row)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Often, we'll have a matrix *x* of input variables and a vector *y* of output variables. In that case, we need to make sure to put corresponding values together in either the training data or the test data:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(x,y,test_pct):\n",
    "    data = zip(x,y)\n",
    "    train, test = split_data(data,1-test_pct)\n",
    "    x_train, y_train = zip(*train)\n",
    "    x_test, y_test = zip(*test)\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"If the model was overfit to the training data, then it will hopefully perform really porrly on the (completely separate) test data.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"However, there are a couple of ways this can go wrong.\"\n",
    "* \"The first is if there are common patterns in the test and train data that wouldn't generalize to a larger data set.\"\n",
    "    * \"For example, imagine that your data set consists of user activity, one row per user per week. In such a case, most user will appear in both the training data and the test data, and certain models might learn to *identify* user rather than discover relationships involving *attributes*.\"\n",
    "* \"A bigger problem is if you use the test/train split not just to judge a model but also to *choose* from among many models.\"\n",
    "    * \"In that case, although each individual model may not be overfit, the 'choose a momdel that performs best on the test set' is a meta-training that makes the test set function as a second training set.\"\n",
    "    * \"In such a situation, you should split the data into three parts: a *training* set for building models, a *validation* set for choosing among trained models, and a *test* set for judging the final model.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Imagine building a model to make a *binary* judgment: 'Is this email spam?'\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Given a set of labeled data and such predictive model, every data point lies in one of four categories:\n",
    "\n",
    "* True positive: 'The message is spam, and we correctly predicted spam';\n",
    "* False positive (Type I Error): 'This message is not spam, but we predicted spam';\n",
    "* False negative (Type II Error): 'This message os spam, but we predicted not spam';\n",
    "* True negative: 'This message is not spam, and we correctly predicted not spam'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix ####\n",
    "\n",
    "|CM                  | Spam               | Not Spam           |\n",
    "|--------------------|--------------------|--------------------|\n",
    "| predicted Spam     | True positive      | False positive     |\n",
    "| predicted Not Spam | False negative     | True negative      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(tp,fp,fn,tn):\n",
    "    correct = tp + tn\n",
    "    total = tp + fp + fn + tn\n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98114\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(70,4930,13930,981070))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"That seems like a pretty impressive number. But clearly this is not a good test, which means that we probably shouldn't put a lot of credence in raw accuracy.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"It's common to look at the combination of *precision* and *recall*.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Precision measures how accurate our positive predicitions were: \"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp,fp,fn,tn):\n",
    "    return tp/(tp+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014\n"
     ]
    }
   ],
   "source": [
    "print(precision(70,4930,13930,981070))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"And recall measures what fraction of the positives our model identified: \"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(tp,fp,fn,tn):\n",
    "    return tp/(tp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005\n"
     ]
    }
   ],
   "source": [
    "print(recall(70,4930,13930,981070))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Sometimes precision and recall are combined into the **F1 score**, which is defined as: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(tp,fp,fn,tn):\n",
    "    p = precision(tp,fp,fn,tn)\n",
    "    r = recall(tp,fp,fn,tn)\n",
    "\n",
    "    return 2 * p * r / (p + r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00736842105263158\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(70,4930,13930,981070))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"This is the *harmonic mean* of precision and recall and necessarily lies between them.\"** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Usually the choice of a model involves a trade-off between precision and recall.\n",
    "* A model that predicts 'yes' when it's even a little bit confident will probably have a high recall but a low precision;\n",
    "* a model that predicts 'yes' only when it's extremely confident is likely to have a low recall and a high precision.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Alternatively, you can think of this as a trade-off between false positives and false negatives:\n",
    "* Saying 'yes' too often will give you lots of false positives;\n",
    "* saying 'no' too often will give you lots of false negatives.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Bias-Variance Trade-off ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"If your model has high bias (which means it performs poorly even on your training data) then one thing to try e adding more features.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"If your model has high variance, then you can similarly remove features. But another solution is to obtain more data (if you can).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction and Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features: \"whatever inputs we provide to our model.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"when your data doesn't have enough features, your model is likely to underfit. And when your data has too many features, it's easy to overfit. But what are features and where do they come from?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In the simplest case, features are simply given to you. If you want to predict someone's salary based on her years of experience, then years of experience is the only feature you have.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Things become more interesting as your data becomes more complicated. Imagine trying to build a spam filter to predict whether an email is junk or not. Most models won't know what to do with a raw email, which is just a collection of text. You'll have to extract features. For example:\n",
    "\n",
    "* Does the email contain the word 'Viagra'?\n",
    "* How many times does the letter 'd' appear?\n",
    "* What was the domain of the sender?\n",
    "\n",
    "The first is simply a yes or no, which we typically encode as a 1 or 0.\n",
    "The second is a number.\n",
    "And the thirs is a choice from a discrete set of options.\n",
    "\n",
    "**Pretty much always, we'll extract features from our data that fall into one of these three categories. What's more, the type of features we have constrains the type of models we can use.**\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"How do we choose features? That's where a combination of *experience* and *domain expertise* comes into play.\"\n",
    "\n",
    "* \"If you've received lots of emails, then you probably have a sense that the presence of certain words might be a good indicator of spamminess.\"\n",
    "* \"And you might also have a sense that the number of d's is likely not a good indicator of spamminess.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
