{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear model: $y_i = \\beta_0 + \\beta_1 \\cdot x_{i1} + \\beta_2 \\cdot x_{i2} + \\varepsilon_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"We’d like for our predicted outputs to be 0 or 1, to indicate class membership. It’s fine if they’re between 0 and 1, since we can interpret these as probabilities — an output of 0.25 could mean 25% chance of being a paid member. But the outputs of the linear model can be huge positive numbers or even negative numbers, which it’s not clear how to interpret. Indeed, here a lot of our predictions were negative.\"\n",
    "- \"The linear regression model assumed that the errors were uncorrelated with the columns of x. \\[...\\] But we know that the actual values must be at most 1, which means that necessarily very large outputs \\[...\\] correspond to very large negative values of the error term. Because this is the case, our estimate of beta is biased.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"*What we’d like instead is for large positive values of dot(x_i, beta) to correspond to probabilities close to 1, and for large negative values to correspond to probabilities close to 0. We can accomplish this by applying another function to the result.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Logistic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def logistic(x):\n",
    "    return 1.0 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7535170b38>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "xs = [i/100. for i in range(-1000,1000)]\n",
    "ys = [logistic(x) for x in xs]\n",
    "\n",
    "plt.plot(xs,ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"As its input gets large and positive, it gets closer and closer to 1. As its input gets large and negative, it gets closer and closer to 0. Additionally, it has the convenient property that its derivative is given by:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_prime(x):\n",
    "    return logistic(x) * (1 - logistic(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use thisto fit the model:\n",
    "\n",
    "$y_i = f\\big(x_i \\cdot \\beta \\big) + \\varepsilon_i$\n",
    "\n",
    "where $f(\\cdot)$ is the logistic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Recall that for linear regression we fit the model by minimizing the sum of squared errors, which ended up choosing the β that maximized the likelihood of the data.\n",
    "\n",
    "Here the two aren’t equivalent, so we’ll use gradient descent to maximize the likelihood directly. This means we need to calculate the likelihood function and its gradient.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Given some $\\beta$, our model says that each $y_i$ should equal $1$ with probability $f\\big(x_i\\ \\beta\\big)$ and $0$ with probability $1 - f\\big(x_i\\ \\beta\\big)$.\n",
    "\n",
    "In particular, the p.d.f. for $y_i$ can be written as:\n",
    "\n",
    "$p\\big(y_i | x_i\\ , \\beta \\big) = f\\big(x_i\\ \\beta\\big)^{y_i}\\ \\big[1 - f\\big(x_i\\ \\beta\\big)\\big]^{1-y_i}$\n",
    "\n",
    "since if $y_i$ is $0$, this equals:\n",
    "\n",
    "$1 - f\\big(x_i\\ \\beta\\big)$\n",
    "\n",
    "and if $y_i$ is 1, this equals:\n",
    "\n",
    "$f\\big(x_i\\ \\beta\\big)$. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"It turns out that it’s actually simpler to maximize the *log likelihood*:\"\n",
    "\n",
    "$\\log L (\\beta | x_i, y_i) = y_i\\ \\log f\\big(x_i\\ \\beta \\big) + (1-y_i)\\ \\log \\big[1 - f\\big(x_i\\ \\beta \\big)\\big]$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Because log is strictly increasing function, any beta that maximizes the log likelihood also maximizes the likelihood, and vice versa.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(v, w):\n",
    "    \"\"\"\n",
    "    v_1 * w_1 + ... + v_n * w_n\n",
    "    \"\"\"\n",
    "    return sum(v_i * w_i \n",
    "               for v_i, w_i in zip(v, w))\n",
    "\n",
    "def logistic_log_likelihood_i(x_i, y_i, beta):\n",
    "    if y_i == 1:\n",
    "        return math.log(logistic(dot(x_i, beta)))\n",
    "    else:\n",
    "        return math.log(1 - logistic(dot(x_i, beta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"If we assume different data points are independent from one another, the overall likelihood is just the product of the individual likelihoods. Which means the overall log likelihood is the sum of the individual log likelihoods:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_log_likelihood(x, y, beta):\n",
    "    return sum(logistic_log_likelihood_i(x_i, y_i, beta)\n",
    "               for x_i, y_i in zip(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_log_partial_ij(x_i, y_i, beta, j):\n",
    "    \"\"\"\n",
    "    here i is the index of the data point,\n",
    "    j the index of the derivative\n",
    "    \"\"\"\n",
    "    return (y_i - logistic(dot(x_i, beta))) * x_i[j]\n",
    "\n",
    "def logistic_log_gradient_i(x_i, y_i, beta):\n",
    "    \"\"\"\n",
    "    the gradient of the log likelihood\n",
    "    corresponding to the ith data point\n",
    "    \"\"\"\n",
    "    return [logistic_log_partial_ij(x_i, y_i, beta, j)\n",
    "            for j, _ in enumerate(beta)]\n",
    "\n",
    "def logistic_log_gradient(x, y, beta):\n",
    "    return reduce(vector_add,\n",
    "                  [logistic_log_gradient_i(x_i, y_i, beta)\n",
    "                   for x_i, y_i in zip(x,y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goodness of fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~\n",
    "true_positives = false_positives = true_negatives = false_negatives = 0\n",
    "for x_i, y_i in zip(x_test, y_test):\n",
    "    predict = logistic(dot(beta_hat, x_i))\n",
    "    if y_i == 1 and predict >= 0.5:\n",
    "        true_positives += 1\n",
    "    elif y_i == 1:\n",
    "        false_negatives += 1\n",
    "    elif predict >= 0.5:\n",
    "        false_positives += 1\n",
    "    else:\n",
    "        true_negatives += 1\n",
    "\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The set of points where *dot(beta_hat, x_i)* equals 0 is the boundary between our classes.\"\n",
    "\n",
    "- \"This boundary is a hyperplane that splits the parameter space into two half-spaces \\[...\\]. We found it as a side-effect of finding the most likely logistic model.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"**An alternative approach to classification is to just look for the hyperplane that 'best' separates the classes in the training data. This is the idea behind the support vector machine, which finds the hyperplane that maximizes the distance to the nearest point in each class**\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"Finding such a hyperplane is an optimization problem that involves techniques that are too advanced for us.\"\n",
    "- \"A different problem is that a separating hyperplane might not exist at all.\"\n",
    "    - \"We can (sometimes) get around this by transforming the data into a higher-dimensional space.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
