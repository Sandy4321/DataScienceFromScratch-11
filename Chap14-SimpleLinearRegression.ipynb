{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y_i = \\beta\\cdot x_i + \\alpha + \\varepsilon_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(alpha, beta, x_i):\n",
    "    return beta * x_i + alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(alpha, beta, x_i, y_i):\n",
    "    \"\"\"\n",
    "    the error from predicting beta * x_i + alpha\n",
    "    when the actual value is y_i\n",
    "    \"\"\"\n",
    "    return y_i - predict(alpha, beta, x_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squared_errors(alpha, beta, x, y):\n",
    "    return sum(error(alpha, beta, x_i, y_i) ** 2 \n",
    "               for x_i, y_i in zip(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The *least squares* solution is to choose the alpha and beta that make sum_of_squared_errors as small as possible.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def dot(v, w):\n",
    "    \"\"\"\n",
    "    v_1 * w_1 + ... + v_n * w_n\n",
    "    \"\"\"\n",
    "    return sum(v_i * w_i \n",
    "               for v_i, w_i in zip(v, w))\n",
    "\n",
    "def mean(x):\n",
    "    return sum(x) / len(x)\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "def variance(x):\n",
    "    \"\"\"\n",
    "    assumes x has at least two elements\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    deviations = de_mean(x)\n",
    "    return sum_of_squares(deviations) / (n - 1)\n",
    "\n",
    "def standard_deviation(x):\n",
    "    return math.sqrt(variance(x))\n",
    "\n",
    "def de_mean(x):\n",
    "    \"\"\"\n",
    "    translate x by subtracting its mean (so the result has mean 0)\n",
    "    \"\"\"\n",
    "    x_bar = mean(x)\n",
    "    return [x_i - x_bar for x_i in x]\n",
    "\n",
    "def covariance(x, y):\n",
    "    n = len(x)\n",
    "    return dot(de_mean(x), de_mean(y)) / (n - 1)\n",
    "\n",
    "def correlation(x, y):\n",
    "    stdev_x = standard_deviation(x)\n",
    "    stdev_y = standard_deviation(y)\n",
    "    if stdev_x > 0 and stdev_y > 0:\n",
    "        return covariance(x, y) / stdev_x / stdev_y\n",
    "    else:\n",
    "        return 0 # if no variation, correlation is zero\n",
    "\n",
    "def least_squares_fit(x, y):\n",
    "    \"\"\"\n",
    "    given training values for x and y,\n",
    "    find the least-squares values of alpha and beta\n",
    "    \"\"\"\n",
    "    beta = correlation(x, y) * standard_deviation(y) / standard_deviation(x)\n",
    "    alpha = mean(y) - beta * mean(x)\n",
    "    return alpha, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"The choice of alpha simply says that when we see the average value of the independent variable x , we predict the average value of the dependent variable y\".\n",
    "* \"The choice of beta means that when the input value increases by standard_deviation(x) , the prediction increases by correlation(x, y) * standard_deviation(y)\".\n",
    "    - \"In the case when x and y are perfectly correlated, a one standard deviation increase in x results in a one-standard-deviation-of- y increase in the prediction.\"\n",
    "    - \"When they’re perfectly anticorrelated, the increase in x results in a decrease in the prediction.\"\n",
    "    - \"And when the correlation is zero, beta is zero, which means that changes in x don’t affect the prediction at all.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Of course, we need a better way to figure out how well we’ve fit the data than staring at the graph. A common measure is the coefficient of determination (or R-squared), which measures the fraction of the total variation in the dependent variable that is\n",
    "captured by the model:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_sum_of_squares(y):\n",
    "    \"\"\"\n",
    "    the total squared variation of y_i's from their mean\n",
    "    \"\"\"\n",
    "    return sum(v ** 2 for v in de_mean(y))\n",
    "\n",
    "def r_squared(alpha, beta, x, y):\n",
    "    \"\"\"\n",
    "    the fraction of variation in y captured by the model, \n",
    "    which equals 1 - the fraction of variation in y not captured \n",
    "    by the model\n",
    "    \"\"\"\n",
    "    return 1.0 - (sum_of_squared_errors(alpha, beta, x, y) / \n",
    "                  total_sum_of_squares(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Now, we chose the alpha and beta that minimized the sum of the squared prediction errors.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"One linear model we could have chosen is “always predict mean(y) ” (corresponding to alpha = mean(y) and beta = 0 ), whose sum of squared errors exactly equals its total sum of squares.\n",
    "* This means an R-squared of zero, which indicates a model that (obviously, in this case) performs no better than just predicting the mean.\"\n",
    "* \"Clearly, the least squares model must be at least as good as that one, which means that the sum of the squared errors is at most the total sum of squares, which means that the R-squared must be at least zero. And the sum of squared errors must be at least 0, which means that the R-squared can be at most 1.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"If we write theta = \\[alpha, beta\\] , then we can also solve this using gradient descent:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_error(x_i, y_i, theta):\n",
    "    alpha, beta = theta\n",
    "    return error(alpha, beta, x_i, y_i) ** 2\n",
    "\n",
    "def squared_error_gradient(x_i, y_i, theta):\n",
    "    alpha, beta = theta\n",
    "    return [-2 * error(alpha, beta, x_i, y_i), # alpha partial derivative\n",
    "            -2 * error(alpha, beta, x_i, y_i) * x_i] # beta partial derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
